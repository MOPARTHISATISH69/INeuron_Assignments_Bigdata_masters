# -*- coding: utf-8 -*-
"""1 Spark RDD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1uGBSF_6xUGxYsAWM0K6sjVXxEUpoSsXI
"""

!pip install pyspark

from pyspark import SparkContext, SparkConf
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
sc = SparkContext.getOrCreate()

data = [1,2,3,4,5]
rDD=sc.parallelize(data,4)
rDD

A=sc.parallelize([2,4,7])

L=A.collect()
print(type(L))
print(L)

A.reduce(lambda x,y:x*y)

rdd =sc.parallelize([5,3,1,2])
rdd.takeOrdered(3,lambda s: 1 * s)

rdd.count()

rdd.saveAsTextFile("/FileStore/rdddata/")

words=['this','is','the','best','mac','ever']
wordRDD=sc.parallelize(words)
wordRDD.reduce(lambda w,v: w if len(w)<len(v) else v)

B=sc.parallelize([1,3,5,2])
B.reduce(lambda x,y: x-y)

def largerThan(x,y):
    if len(x)>len(y): return x
    elif len(y)>len(x): return y
    else:  #lengths are equal, compare lexicographically
        if x>y: 
            return x
        else: 
            return y
        
wordRDD.reduce(largerThan)

A=sc.parallelize(range(100000))
print(A.getNumPartitions())

D=A.repartition(6)
print(D.getNumPartitions())

D=A.coalesce(4)
print(D.getNumPartitions())

A=sc.parallelize(range(1000000),numSlices=12)
print(A.getNumPartitions())

rdd=sc.parallelize([1,2,3,4])
rdd.map(lambda x:x*2).collect()

rdd.filter(lambda x:x%2==0).collect()

rdd2=sc.parallelize([1,4,2,2,3])
rdd2.distinct().collect()

n=1000000
B=sc.parallelize([1,2,3,4]*int(n/4))

def greaterthan(x):
  return x > 1
print('the number of elements in B that are > 3 =',B.filter(greaterthan).count())

print('the number of elements in B that are > 3 =',B.filter(lambda n: n > 1).count())

# Remove duplicate element in DuplicateRDD, we get distinct RDD
DuplicateRDD = sc.parallelize([1,1,2,2,3,3])
print('DuplicateRDD=',DuplicateRDD.collect())
print('DistinctRDD = ',DuplicateRDD.distinct().collect())

text=["you are my sunshine","my only sunshine"]
text_file = sc.parallelize(text)
# map each line in text to a list of words
print('map:',text_file.map(lambda line: line.split(" ")).collect())
# create a single list of words by combining the words from all of the lines
print('flatmap:',text_file.flatMap(lambda line: line.split(" ")).collect())

rdd1 = sc.parallelize([1, 1, 2, 3])
rdd2 = sc.parallelize([1, 3, 4, 5])

rdd2=sc.parallelize(['a','b',1])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print('union as bags =',rdd1.union(rdd2).collect())
print('union as sets =',rdd1.union(rdd2).distinct().collect())

regular_rdd = sc.parallelize([1, 2, 3, 4, 2, 5, 6])
pair_rdd = regular_rdd.map( lambda x: (x, x*x) )
print(pair_rdd.collect())

rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.reduceByKey(lambda a,b: a+b).collect())

rdd = sc.parallelize([(2,2), (1,4), (3,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.sortByKey().collect())

rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
print("After transformation : ", rdd.mapValues(lambda x: x*x).collect())

rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
#print("After transformation : ", rdd.groupByKey().mapValues(lambda x:[a for a in x]).collect())
print("After transformation : ", rdd.groupByKey().mapValues(lambda x:[a+a for a in x]).collect())

rdd = sc.parallelize([(1,2), (2,4), (2,6)])
print("Original RDD :", rdd.collect())
# the lambda function generates for each number i, an iterator that produces i,i+1
print("After transformation : ", rdd.flatMapValues(lambda x: list(range(x,x+2))).collect())

rdd1 = sc.parallelize([(1,2),(2,1),(2,2)])
rdd2 = sc.parallelize([(2,5),(3,1)])
print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.leftOuterJoin(rdd2).collect())

print('rdd1=',rdd1.collect())
print('rdd2=',rdd2.collect())
print("Result:", rdd1.join(rdd2).collect())

